{"main_prompt": "==\n\nintro / persona\n\nYou are a security researcher in a cyber security company with extensive Python and Docker knowledge. \n\nYou are a worker as part of a process of researching a new security control and integrating it as part of the platform \n\n==\n\n==\n\nenv + interface\n\nYou have access to a mac terminal with docker installed.\n\nYou will interact with me to use it and achieve your task. \n\nYou will have the history of our conversation so that you can continue from the last point.\n\nYou can ask me to do the following commands:\n\n1. Run command - I will run a shell command in the environment and return the output to you.\n2. Send content of file - I will provide you with the contents of a local file\n3. Write content to file - I will write the content you provide to a file you ask me to\n\nYou will receive a main objective and you can divide to tasks if needed. Once you believe you completed a task, you can do what is needed to decide if the objective was completed or not (for example - ask me for an output of a file that contains the result of the task)\n\nYour response will be in the following structure:\n\n```json\n{\n\t\"current_objective\": \"...\",\n\t\"required_task\": \"\" // The task you require me to do, if needed (if not - send null)\n\t\"task_input\": {\"file_name\": \"file.txt\", \"file_content\": \"...\"} // The input of the task, can be one or more variables depending on the task, always an object. \n}\n```\n\nRequired task can be: run_command, send_file_content, write_content_to_file\n\nValid task input objects are:\n\n{\u201dcommand\u201d: \u201cls -l\u201d } - Run command\n\n{\u201dfile_name\u201d: \u201cfile.txt\u201d} - Send content\n\n{\"file_name\": \"file.txt\", \"file_content\": \"...\"} - Write content to file\n\nBe sure to use the command ls -lah, in order to find where you are in case you get stuck.\n\nOnce you determined that you successfully completed the overall objective you were provided (not a single task), send the following json message:\n\n```json\n {\n\t \"objective_status\": \"completed\",\n\t \"text_output\": \"..Explanation of what was done and what was achieved on the overall objective\",\n\t \"files_map\": {\n\t\t\t \"Dockerfile\": \"The dockerfile for the container\",\n\t\t\t \"example.py\": \"The main Python file that wraps the binary\",\n\t\t\t \"code/file.txt\":\"The example file that includes the findings\"\n\t\t\t \n\t\t}\n  } \n```\n\nIn the files map, we expect a list of relevant files that will be transferred to future tasks. You will provide the names of the files you created and for each file a description of what it does and what you did (not the file content). The file names will be the relevant path from the starting directory. \n\n== \n\n==\n\ndocker instructions\n\nWhen you\u2019re testing containers, be sure to note the following:\n\n- If there are multiple output destinations (for example - /code and /tmp), be sure to create local directories that will be mounted to those locations, so that you can later inspect their content,\n- In continuation for that - use the \u2014rm command and do not leave containers hanging unless it\u2019s necessary. Run the container with all the relevant mounts and see their output once the container closes.\n- Also, do not mount your starting directory straight to the container. As mentioned before, create directories for the container and mount them.\n- If you discovered that your Dockerfile was not sufficient, it\u2019s OK to rewrite it and run the container again\nIN ADDITION, When you build dockerfiles, always use the /app directory for the code you put in the container, and make sure to always use ENTRYPOINT instead of CMD.\n==\n\n==\n\nImportant rule\n\n- In your response, you may add a small text above the JSON with your thoughts, but end of each response will be a ONE SINGLE json according to the instructions, no more\n- When you edit an existing file content, I expect you to write to me the entire file content, not just parts to edit - the reason for that is that I take the code as is and override the relevant file content with the code you give me, or else I would not know how to edit it.", "step_instructions": [{"step_objective": "Create a Dockerfile that wraps the control", "step_prompt": "Your objective is to wrap the Control in a Docker image. In your environment you have:\n\n- The Control executable\n- Docker installed and running\n\nYou will use the Ubuntu 22.04 Image.\n\nYour goal is to build a Docker image that upon setting up the container, runs the Control on the `/code` directory. It will then make sure that the output is written to the `/tmp` directory. The image will show the output of the Control if its logs are inspected.  You must make sure it works by checking that the docker image properly finds the provided example finding, and that a valid JSON result file is created.\n\nYou will create the image and verify that it works properly. This is the definition of your objective.", "model": "gpt-4-turbo", "output": {"objective_status": "completed", "text_output": "Created and tested a Docker configuration to run Checkov scans. Successfully observed and validated output for IaC assessments, demonstrating the operational setup of the container to perform security checks on provided configurations.", "files_map": {"Dockerfile": "The dockerfile for the container", "entrypoint.py": "The main Python file that executes the Checkov binary within the Docker container and ensures outputs are directed correctly."}}}, {"step_objective": "Wrap the control in a python script", "step_prompt": "Your objective is to wrap the execution of the tool with a Python script. The script will be very simple and will only run the required command to execute the tool, nothing else. The desired outcome is a modified Dockerfile and a modified container that properly runs the tool with exactly the same output as before.\n\nThe script should write nothing additional to stdout.  I expect in the output to have both the Dockerfile and the relevant Python script file. Continue using the same base image and do not change it. Read the existing Dockerfile so that you know how to continue it.\n\nWhen you write the python script, please make sure to use `subprocess` to run commands, and keep clean code principles. Be sure to divide it to functions and make the code production ready. Note that `subprocess` is part of the standard library and does not need to be installed.\n\nI expect the python script file to be in the main current directory. Any sub directories are only used for examples.\n\nYou must check yourself with the provided example and make sure the tool properly finds secrets and writes them to a JSON file before declaring the objective as complete.", "model": "gpt-4-turbo", "output": {"objective_status": "completed", "text_output": "Wrapped the execution of Checkov within a Python script and successfully integrated into the Docker container. The container was rebuilt to reflect script changes, ensuring the execution of Checkov scans the IaC files and outputs results without additional stdout mishandlings. The Checkov tool functioned as expected, identifying both passing and failing checks on the provided IaC sample.", "files_map": {"Dockerfile": "The Dockerfile set up the environment, installed necessary packages including Checkov, and specified the entrypoint script.", "entrypoint.py": "The main Python file that wraps the execution of the Checkov command without creating additional stdout outputs."}}}, {"step_objective": "Jit finding json schema", "step_prompt": "Here is the pydantic model for a Jit finding:\n\nfrom enum import Enum\nfrom pydantic import BaseModel\nfrom typing import List, Optional, Dict\n\nclass Severity(str, Enum):\n    CRITICAL = \"CRITICAL\"\n    HIGH = \"HIGH\"\n    MEDIUM = \"MEDIUM\"\n    LOW = \"LOW\"\n\nclass RawControlFinding(BaseModel):\n    \"\"\"\n    Finding that is received from the control\n    \"\"\"\n\n    test_name: str \n    fingerprint: str # unique ID that will always identifiy this specific finding\n    test_id: str # An ID for a finding of this type \n    issue_text: Optional[str] # A description of the finding\n    issue_confidence: str = 'UNDEFINED' # not used really\n    issue_severity: Severity\n    references: Optional[List[Dict[Optional[str], Optional[str]]]] = []\n    location: Optional[str] = None\n    location_text: Optional[str] = None\n\nclass CodeRawControlFinding(RawControlFinding):\n    \"\"\"\n    Code Findings from app sec controls\n    \"\"\"\n    filename: Optional[str]\n    line_range: Optional[str]\n    code_snippet: Optional[str]\n\nHere is an example for such a finding (not from the tool you need):\n\n```json\n\n{\n  \"test_name\": \"python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\",\n  \"fingerprint\": \"22938f4e0e9139feff931c4655a6ce9d3ac2e2219f30aa64867b835d253df473\",\n  \"test_id\": \"python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\",\n  \"issue_text\": \"Found 'subprocess' function 'check_output' with 'shell=True'. This is dangerous because this call will spawn the command using a shell process. Doing so propagates current shell settings and variables, which makes it much easier for a malicious actor to execute commands. Use 'shell=False' instead.\",\n  \"issue_confidence\": \"UNDEFINED\",\n  \"issue_severity\": \"HIGH\",\n  \"references\": [\n    {\n      \"name\": \"https://semgrep.dev/r/python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\",\n      \"url\": \"https://semgrep.dev/r/python.lang.security.audit.subprocess-shell-true.subprocess-shell-true\"\n    }\n  ],\n  \"location\": \"https://github.com/shlomi-test-org/python-code-/blob/6d0d3c9e948f2a5c0aa64098d24b90859f0e507a/asdasdsadds.py#L64-L64\",\n  \"location_text\": \"shlomi-test-org/python-code-\",\n  \"filename\": \"asdasdsadds.py\",\n  \"line_range\": \"64-64\",\n  \"code_snippet\": \"output = subprocess.check_output(f\\\"nslookup {domain}\\\", shell=True, encoding='UTF-8')\"\n}\n\n```\n\nYour objective is to modify the exiting python code to translate the tool's output JSON to a JSON that fits our model.\n\nYou will need to do the following tasks:\n* Read the existing Dockerfile and python file to know what they currently look like\n* If needed, run the container on the provided example and see the output JSON\n* Modify the python file as needed, make sure to map the correct items as needed from the output JSON so that you have a model that fits\n* Make sure the tool's output is now written to /tmp/jit_output.json in the container. \n* As part of the checks, if you dont have them already in the provided tree, make sure to create code and tmp folders and ensure that the code dir has an example that makes the control raise findings. Make sure to map them to the container as required. If you already have them with example files, use them. Make sure to ask to see the tool's JSON output BEFORE to make changes to the script, to make sure you know how to map it properly.\n* Take the output JSON you created, and write a small Python script that loads this JSON and tries loading it into the provided Pydantic model. Make sure it doesn't crash and works properly.\n* When you are writing the script, note that many tool have exit code 1, but their output JSON will still be created. Your code needs to handle this properly and still continue to the conversion if the output JSON was created successfully, despite the exist code 1\n\n. Be sure to install Pydantic in the Dockerfile, and when you write the python file, copy the entire models to the files. It cannot be imported from anywhere. In the output, I expect the modified Dockerfile, Python file,  The output example.", "model": "gpt-4-turbo"}, {"step_objective": "Weap with entrypoint", "step_prompt": "We have an entrypoint executable whose job is to read the tool's output and do things with it, such as uploading the findings to our server. Your objective is to the python script with it. The way the entrypoint works is that you run is like this:./entrypoint <command>Before running the entrypoint, you need to make sure the SECURITY_CONTROL_OUTPUT_FILE is set to file named \"/tmp/jit_output.json\". After the entrypoint is run properly. you will need to read this file and make sure the JSON there is written properly. This needs to be done as part of the container.You must check that the tool not only ran but actually found findings. Use the provided example/s for this as needed. BE sure to mount both the result and code example directories to the container when you test it.IN addition to the log file, you are expected to check the container's logs and make sure the following text is found there:Uploading Findings to serverIf both of these are available, it means the entrypoint works properly.You have the Dockerfile, the relevant Python files in your local directory. Be sure to read them if needed. Note that `entrypoint` is a binary file. \n Note that in this part, you do not need to make any changes in the python file/s. I expect that in the end, the Dockerfile will have an ENTRYPOINT line that begins with the entrypoint executable and run the existing python code. So something like ENTRYPOINT [\"/app/entrypoint\", \"...python file execution commands\"]", "model": "gpt-4-turbo"}], "research_prompt": "Our Checkhov research:\n\nCheckov must run in a valid Git repo, otherwise it finds nothing.\n\nHere is the checkov help:\n\n```json\ncheckov iac scans code, past or present, for secrets\n\nusage: checkov [-h] [-v] [--support] [-d DIRECTORY] [--add-check] [-f FILE [FILE ...]] [--skip-path SKIP_PATH] [--external-checks-dir EXTERNAL_CHECKS_DIR] [--external-checks-git EXTERNAL_CHECKS_GIT] [-l]\n               [-o {cli,csv,cyclonedx,cyclonedx_json,json,junitxml,github_failed_only,gitlab_sast,sarif,spdx}] [--output-file-path OUTPUT_FILE_PATH] [--output-bc-ids] [--include-all-checkov-policies] [--quiet] [--compact] [--framework FRAMEWORK [FRAMEWORK ...]]\n               [--skip-framework SKIP_FRAMEWORK [SKIP_FRAMEWORK ...]] [-c CHECK] [--skip-check SKIP_CHECK] [--run-all-external-checks] [-s] [--soft-fail-on SOFT_FAIL_ON] [--hard-fail-on HARD_FAIL_ON] [--bc-api-key BC_API_KEY] [--prisma-api-url PRISMA_API_URL] [--skip-results-upload]\n               [--docker-image DOCKER_IMAGE] [--dockerfile-path DOCKERFILE_PATH] [--repo-id REPO_ID] [-b BRANCH] [--skip-download] [--use-enforcement-rules] [--download-external-modules DOWNLOAD_EXTERNAL_MODULES] [--var-file VAR_FILE]\n               [--external-modules-download-path EXTERNAL_MODULES_DOWNLOAD_PATH] [--evaluate-variables EVALUATE_VARIABLES] [-ca CA_CERTIFICATE] [--no-cert-verify] [--repo-root-for-plan-enrichment REPO_ROOT_FOR_PLAN_ENRICHMENT] [--config-file CONFIG_FILE] [--create-config CREATE_CONFIG]\n               [--show-config] [--create-baseline] [--baseline BASELINE] [--output-baseline-as-skipped] [--skip-cve-package SKIP_CVE_PACKAGE] [--policy-metadata-filter POLICY_METADATA_FILTER] [--policy-metadata-filter-exception POLICY_METADATA_FILTER_EXCEPTION]\n               [--secrets-scan-file-type SECRETS_SCAN_FILE_TYPE] [--enable-secret-scan-all-files] [--block-list-secret-scan BLOCK_LIST_SECRET_SCAN] [--summary-position {bottom,top}] [--skip-resources-without-violations] [--deep-analysis] [--no-fail-on-crash] [--mask MASK]\n               [--scan-secrets-history] [--secrets-history-timeout SECRETS_HISTORY_TIMEOUT] [--openai-api-key OPENAI_API_KEY]\n\nInfrastructure as code static analysis\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -v, --version         version\n  --support             Enable debug logs and upload the logs to the server. Requires a Bridgecrew or Prisma Cloud API key.\n  -d DIRECTORY, --directory DIRECTORY\n                        IaC root directory (can not be used together with --file).\n  --add-check           Generate a new check via CLI prompt\n  -f FILE [FILE ...], --file FILE [FILE ...]\n                        File to scan (can not be used together with --directory). With this option, Checkov will attempt to filter the runners based on the file type. For example, if you specify a \".tf\" file, only the terraform and secrets frameworks will be included. You can further\n                        limit this (e.g., skip secrets) by using the --skip-framework argument.\n  --skip-path SKIP_PATH\n                        Path (file or directory) to skip, using regular expression logic, relative to current working directory. Word boundaries are not implicit; i.e., specifying \"dir1\" will skip any directory or subdirectory named \"dir1\". Ignored with -f. Can be specified multiple\n                        times.\n  --external-checks-dir EXTERNAL_CHECKS_DIR\n                        Directory for custom checks to be loaded. Can be repeated\n  --external-checks-git EXTERNAL_CHECKS_GIT\n                        Github url of external checks to be added. you can specify a subdirectory after a double-slash //. possible to use ?ref=tags/tagName or ?ref=heads/branchName or ?ref=commit_id cannot be used together with --external-checks-dir\n  -l, --list            List checks\n  -o {cli,csv,cyclonedx,cyclonedx_json,json,junitxml,github_failed_only,gitlab_sast,sarif,spdx}, --output {cli,csv,cyclonedx,cyclonedx_json,json,junitxml,github_failed_only,gitlab_sast,sarif,spdx}\n                        Report output format. Add multiple outputs by using the flag multiple times (-o sarif -o cli)\n  --output-file-path OUTPUT_FILE_PATH\n                        Name of the output folder to save the chosen output formats. Advanced usage: By using -o cli -o junitxml --output-file-path console,results.xml the CLI output will be printed to the console and the JunitXML output to the file results.xml.\n  --output-bc-ids       Print Bridgecrew platform IDs (BC...) instead of Checkov IDs (CKV...), if the check exists in the platform\n  --include-all-checkov-policies\n                        When running with an API key, Checkov will omit any policies that do not exist in Prisma Cloud platform, except for local custom policies loaded with the --external-check flags. Use this key to include policies that only exist in Checkov in the scan. Note that\n                        this will make the local CLI results different from the results you see in the platform. Has no effect if you are not using an API key. Use the --check option to explicitly include checks by ID even if they are not in the platform, without using this flag.\n  --quiet               in case of CLI output, display only failed checks. Also disables progress bars\n  --compact             in case of CLI output, do not display code blocks\n  --framework FRAMEWORK [FRAMEWORK ...]\n                        Filter scan to run only on specific infrastructure as code frameworks. Defaults to all frameworks. If you explicitly include 'all' as a value, then all other values are ignored. Enter as a comma-separated list or repeat the flag multiple times. For example,\n                        --framework terraform,sca_package or --framework terraform --framework sca_package. Possible values: all, ansible, argo_workflows, arm, azure_pipelines, bicep, bitbucket_pipelines, cdk, circleci_pipelines, cloudformation, dockerfile, github_configuration,\n                        github_actions, gitlab_configuration, gitlab_ci, bitbucket_configuration, helm, json, yaml, kubernetes, kustomize, openapi, sca_package, sca_image, secrets, serverless, terraform, terraform_json, terraform_plan, sast, sast_python, sast_java, sast_javascript,\n                        3d_policy [env var: CKV_FRAMEWORK]\n  --skip-framework SKIP_FRAMEWORK [SKIP_FRAMEWORK ...]\n                        Filter scan to skip specific infrastructure as code frameworks. This will be included automatically for some frameworks if system dependencies are missing. Enter as a comma-separated list or repeat the flag multiple times. For example, --skip-framework\n                        terraform,sca_package or --skip-framework terraform --skip-framework sca_package. Cannot include values that are also included in --framework. Possible values: ansible, argo_workflows, arm, azure_pipelines, bicep, bitbucket_pipelines, cdk, circleci_pipelines,\n                        cloudformation, dockerfile, github_configuration, github_actions, gitlab_configuration, gitlab_ci, bitbucket_configuration, helm, json, yaml, kubernetes, kustomize, openapi, sca_package, sca_image, secrets, serverless, terraform, terraform_json, terraform_plan,\n                        sast, sast_python, sast_java, sast_javascript, 3d_policy\n  -c CHECK, --check CHECK\n                        Checks to run; any other checks will be skipped. Enter one or more items separated by commas. Each item may be either a Checkov check ID (CKV_AWS_123), a BC check ID (BC_AWS_GENERAL_123), or a severity (LOW, MEDIUM, HIGH, CRITICAL). If you use a severity, then\n                        all checks equal to or above the lowest severity in the list will be included. This option can be combined with --skip-check. If it is, then the logic is to first take all checks that match this list, and then remove all checks that match the skip list. For\n                        example, if you use --check CKV_123 and --skip-check LOW, then CKV_123 will not run if it is a LOW severity. Similarly, if you use --check CKV_789 --skip-check MEDIUM, then CKV_789 will run if it is a HIGH severity. If you use a check ID here along with an API\n                        key, and the check is not part of the BC / PC platform, then the check will still be run (see --include-all-checkov-policies for more info). [env var: CKV_CHECK]\n  --skip-check SKIP_CHECK\n                        Checks to skip; any other checks will not be run. Enter one or more items separated by commas. Each item may be either a Checkov check ID (CKV_AWS_123), a BC check ID (BC_AWS_GENERAL_123), or a severity (LOW, MEDIUM, HIGH, CRITICAL). If you use a severity, then\n                        all checks equal to or below the highest severity in the list will be skipped. This option can be combined with --check. If it is, priority is given to checks explicitly listed by ID or wildcard over checks listed by severity. For example, if you use --skip-check\n                        CKV_123 and --check HIGH, then CKV_123 will be skipped even if it is a HIGH severity. In the case of a tie (e.g., --check MEDIUM and --skip-check HIGH for a medium severity check), then the check will be skipped. [env var: CKV_SKIP_CHECK]\n  --run-all-external-checks\n                        Run all external checks (loaded via --external-checks options) even if the checks are not present in the --check list. This allows you to always ensure that new checks present in the external source are used. If an external check is included in --skip-check, it\n                        will still be skipped.\n  -s, --soft-fail       Runs checks but always returns a 0 exit code. Using either --soft-fail-on and / or --hard-fail-on overrides this option, except for the case when a result does not match either of the soft fail or hard fail criteria, in which case this flag determines the result.\n  --soft-fail-on SOFT_FAIL_ON\n                        Exits with a 0 exit code if only the specified items fail. Enter one or more items separated by commas. Each item may be either a Checkov check ID (CKV_AWS_123), a BC check ID (BC_AWS_GENERAL_123), or a severity (LOW, MEDIUM, HIGH, CRITICAL). If you use a\n                        severity, then any severity equal to or less than the highest severity in the list will result in a soft fail. This option may be used with --hard-fail-on, using the same priority logic described in --check and --skip-check options above, with --hard-fail-on\n                        taking precedence in a tie. If a given result does not meet the --soft-fail-on nor the --hard-fail-on criteria, then the default is to hard fail\n  --hard-fail-on HARD_FAIL_ON\n                        Exits with a non-zero exit code for specified checks. Enter one or more items separated by commas. Each item may be either a Checkov check ID (CKV_AWS_123), a BC check ID (BC_AWS_GENERAL_123), or a severity (LOW, MEDIUM, HIGH, CRITICAL). If you use a severity,\n                        then any severity equal to or greater than the lowest severity in the list will result in a hard fail. This option can be used with --soft-fail-on, using the same priority logic described in --check and --skip-check options above, with --hard-fail-on taking\n                        precedence in a tie.\n  --bc-api-key BC_API_KEY\n                        Bridgecrew API key or Prisma Cloud Access Key (see --prisma-api-url) [env var: BC_API_KEY]\n  --prisma-api-url PRISMA_API_URL\n                        The Prisma Cloud API URL (see: https://prisma.pan.dev/api/cloud/api-urls). Requires --bc-api-key to be a Prisma Cloud Access Key in the following format: <access_key_id>::<secret_key> [env var: PRISMA_API_URL]\n  --skip-results-upload\n                        Do not upload scan results to the platform to view in the console. Results are only available locally. If you use the --support flag, logs will still get uploaded.\n  --docker-image DOCKER_IMAGE, --image DOCKER_IMAGE\n                        Scan docker images by name or ID. Only works with --bc-api-key flag\n  --dockerfile-path DOCKERFILE_PATH\n                        Path to the Dockerfile of the scanned docker image\n  --repo-id REPO_ID     Identity string of the repository, with form <repo_owner>/<repo_name>. Required when using the platform integration (API key).\n  -b BRANCH, --branch BRANCH\n                        Selected branch of the persisted repository. Only has effect when using the --bc-api-key flag\n  --skip-download       Do not download any data from Prisma Cloud. This will omit doc links, severities, etc., as well as custom policies and suppressions if using an API token. Note: it will prevent BC platform IDs from being available in Checkov.\n  --use-enforcement-rules\n                        Use the Enforcement rules configured in the platform for hard / soft fail logic. With this option, the enforcement rule matching this repo, or the default rule if there is no match, will determine this behavior: any check with a severity below the selected rule's\n                        soft-fail threshold will be skipped; any check with a severity equal to or greater than the rule's hard-fail threshold will be part of the hard-fail list, and any check in between will be part of the soft-fail list. For example, if the given enforcement rule has\n                        a hard-fail value of HIGH and a soft-fail value of MEDIUM,this is the equivalent of using the flags `--skip-check LOW --hard-fail-on HIGH`. You can use --check, --skip-check, --soft-fail, --soft-fail-on, or --hard-fail-on to override portions of an enforcement\n                        rule. Note, however, that the logic of applying the --check list and then the --skip-check list (as described above under --check) still applies here. Requires a BC or PC platform API key.\n  --download-external-modules DOWNLOAD_EXTERNAL_MODULES\n                        download external terraform modules from public git repositories and terraform registry [env var: DOWNLOAD_EXTERNAL_MODULES]\n  --var-file VAR_FILE   Variable files to load in addition to the default files (see https://www.terraform.io/docs/language/values/variables.html#variable-definitions-tfvars-files).Currently only supported for source Terraform (.tf file), and Helm chart scans.Requires using --directory,\n                        not --file. [env var: CKV_VAR_FILE]\n  --external-modules-download-path EXTERNAL_MODULES_DOWNLOAD_PATH\n                        set the path for the download external terraform modules [env var: EXTERNAL_MODULES_DIR]\n  --evaluate-variables EVALUATE_VARIABLES\n                        evaluate the values of variables and locals [env var: CKV_EVAL_VARS]\n  -ca CA_CERTIFICATE, --ca-certificate CA_CERTIFICATE\n                        Custom CA certificate (bundle) file [env var: BC_CA_BUNDLE]\n  --no-cert-verify      Skip SSL certificate verification. Use this to bypass errors related to SSL certificates. Warning: this should only be used for testing purposes. Skipping certificate verification is dangerous as invalid and falsified certificates cannot be detected.\n  --repo-root-for-plan-enrichment REPO_ROOT_FOR_PLAN_ENRICHMENT\n                        Directory containing the hcl code used to generate a given plan file. Use with -f.\n  --config-file CONFIG_FILE\n                        path to the Checkov configuration YAML file\n  --create-config CREATE_CONFIG\n                        takes the current command line args and writes them out to a config file at the given path\n  --show-config         prints all args and config settings and where they came from (eg. commandline, config file, environment variable or default)\n  --create-baseline     Alongside outputting the findings, save all results to .checkov.baseline file so future runs will not re-flag the same noise. Works only with `--directory` flag\n  --baseline BASELINE   Use a .checkov.baseline file to compare current results with a known baseline. Report will include only failed checks that are new with respect to the provided baseline\n  --output-baseline-as-skipped\n                        output checks that are skipped due to baseline file presence\n  --skip-cve-package SKIP_CVE_PACKAGE\n                        filter scan to run on all packages but a specific package identifier (denylist), You can specify this argument multiple times to skip multiple packages\n  --policy-metadata-filter POLICY_METADATA_FILTER\n                        comma separated key:value string to filter policies based on Prisma Cloud policy metadata. When used with --policy-metadata-filter-exception, the exceptions override any policies selected asa result of the --policy-metadata-filter flag.See\n                        https://prisma.pan.dev/api/cloud/cspm/policy#operation/get-policy-filters-and-options for information on allowed filters. Format: policy.label=test,cloud.type=aws\n  --policy-metadata-filter-exception POLICY_METADATA_FILTER_EXCEPTION\n                        comma separated key:value string to exclude filtered policies based on Prisma Cloud policy metadata. When used with --policy-metadata-filter, the exceptions override any policies selected asa result of the --policy-metadata-filter flag.See\n                        https://prisma.pan.dev/api/cloud/cspm/policy#operation/get-policy-filters-and-options for information on allowed filters. Format: policy.label=test,cloud.type=aws\n  --secrets-scan-file-type SECRETS_SCAN_FILE_TYPE\n                        not in use [env var: CKV_SECRETS_SCAN_FILE_TYPE]\n  --enable-secret-scan-all-files\n                        enable secret scan for all files [env var: CKV_SECRETS_SCAN_ENABLE_ALL]\n  --block-list-secret-scan BLOCK_LIST_SECRET_SCAN\n                        List of files to filter out from the secret scanner [env var: CKV_SECRETS_SCAN_BLOCK_LIST]\n  --summary-position {bottom,top}\n                        Chose whether the summary will be appended on top (before the checks results) or on bottom (after check results), default is on top.\n  --skip-resources-without-violations\n                        exclude extra resources (resources without violations) from report output [env var: CKV_SKIP_RESOURCES_WITHOUT_VIOLATIONS]\n  --deep-analysis       Combine the TF Plan and TF graphs to make connections not available in either\n  --no-fail-on-crash    Return exit code 0 instead of 2 [env var: CKV_NO_FAIL_ON_CRASH]\n  --mask MASK           List of <resource_type>:<variable> OR <variable> only. Each entry in the list will be used formasking the desired attribute for resource (or for all resources, if no resource given).Notice: one entry can contain several variables, seperated with a comma. For\n                        example:<resource_type>:<variable1>,<variable2> OR <variable1>,<variable2>\n  --scan-secrets-history\n                        will scan the history of commits for secrets\n  --secrets-history-timeout SECRETS_HISTORY_TIMEOUT\n                        maximum time to stop the scan\n  --openai-api-key OPENAI_API_KEY\n                        Add an OpenAI API key to enhance finding guidelines by sending violated policies and resource code to OpenAI to request remediation guidance. This will use your OpenAI credits. Set your number of findings that will receive enhanced guidelines using\n                        CKV_OPENAI_MAX_FINDINGS [env var: CKV_OPENAI_API_KEY]\n\nArgs that start with '--' can also be set in a config file (/Users/moshikol/Downloads/dist/.checkov.yaml or /Users/moshikol/Downloads/dist/.checkov.yml or /Users/moshikol/.checkov.yaml or /Users/moshikol/.checkov.yml or specified via --config-file). The config file uses YAML syntax and\nmust represent a YAML 'mapping' (for details, see http://learn.getgrav.org/advanced/yaml). In general, command-line values override environment variables which override config file values which override defaults.\n\n```\n\n---\n\nAn example for a vulnerable iac code is: \n\n```\nresource \"aws_s3_bucket\" \"foo-bucket\" {\n  region        = var.region\n    #checkov:skip=CKV_AWS_20:The bucket is a public static content host\n  bucket        = local.bucket_name\n  force_destroy = true\n  acl           = \"public-read\"\n}\n\n```\n\nExample of Checkov execution:\n\n```json\ncheckov --directory . -o json --output-file-path /tmp\n```\n\nCheckov will create the file results_json.json inside the provided path.\n\nCheckov can be installed using pip:\n\n```json\npip3 install checkov\n```", "research_files": ["/Users/jonathan/Code/jitathon-control-freaks/inputs/gitleaks"], "control_name": "gitleaks", "executable_name": "gitleaks"}